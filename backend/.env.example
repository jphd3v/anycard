# Backend env example
# Copy to backend/.env and adjust as needed.

# HTTP port for the backend
# PORT=3000

# Backend LLM is OPT-IN. Set to true to allow server AI turns; default false disables.
# BACKEND_LLM_ENABLED=false

# OpenAI-compatible endpoint for server-side AI policy (required when enabled).

# E.g. Google has a free tier to try out couple of prompts
# LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# LLM_API_KEY=your-api-key
# LLM_MODEL=gemini-2.0-flash-lite

# Also, OpenRouter has a free tier:
# LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# LLM_MODEL=mistralai/devstral-2512:free
# ...

# Optional tuning
# LLM_TEMPERATURE=0.4
# LLM_TURN_TIMEOUT_MS=30000
# LLM_MIN_THINK_TIME_MS=300

# When true, AI logs are sent to the frontend (candidates, prompts, responses).
# When false, the AI log UI stays empty/closed.
# LLM_DEBUG_HTTP=false

# When true, include detailed error info in AI logs (status codes/response bodies).
# If false, errors are still noted but details are omitted.
# LLM_SHOW_EXCEPTIONS_IN_FRONTEND=true

# LLM_STRIP_OPENAI_DEFAULTS=true

# AI Policy Mode: "llm" (default) or "firstCandidate" for deterministic testing
# LLM_POLICY_MODE=llm

# Comma-separated list of allowed frontend origins
# CLIENT_ORIGIN=http://localhost:5173,http://127.0.0.1:5173
