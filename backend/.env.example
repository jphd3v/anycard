# Backend env example
# Copy to backend/.env and adjust as needed.

# HTTP port for the backend
# PORT=3000

# Comma-separated list of allowed frontend origins
# CLIENT_ORIGIN=http://localhost:5173,http://127.0.0.1:5173

# Backend LLM is OPT-IN. Set to true to allow server AI turns; default false disables.
# BACKEND_LLM_ENABLED=false

# OpenAI-compatible endpoint for server-side AI policy (required when enabled).

# E.g. Google has a free tier to try out couple of prompts
# LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# LLM_API_KEY=your-api-key
# LLM_MODEL=gemini-2.0-flash-lite

# Also, OpenRouter has a free tier:
# LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# LLM_API_KEY=your-api-key
# LLM_MODEL=mistralai/devstral-2512:free

# Optional LLM tuning
# LLM_TEMPERATURE=0.4
# LLM_TURN_TIMEOUT_MS=30000
# LLM_MIN_THINK_TIME_MS=300
# LLM_STRIP_OPENAI_DEFAULTS=true

# When true, the LLM client prints internal request/response logs to the backend terminal.
# LLM_LOGGING_ENABLED=false

# When true, raw backend AI technical data (prompts, responses) is broadcast to the frontend game log.
# When false, only high-level game moves are shown for backend AI.
# (Note: Frontend-run AI logs are always shown since they run in the browser).
# LLM_SHOW_PROMPTS_IN_FRONTEND=false

# When true, include detailed error info in AI logs (status codes/response bodies).
# If false, errors are still noted but details are omitted.
# LLM_SHOW_EXCEPTIONS_IN_FRONTEND=true

# AI Policy Mode: "llm" (default) or "firstCandidate" for deterministic testing
# LLM_POLICY_MODE=llm
