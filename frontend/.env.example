# Example environment variables for the frontend
# Copy to frontend/.env and adjust as needed.

# Set the backend server URL for the frontend to connect to
#VITE_SERVER_URL=http://localhost:3000

# Client-side LLM Configuration (Browser-based AI)
# When enabled, the frontend directly calls the LLM API for seats it sponsors.
#VITE_BROWSER_LLM_ENABLED=false

# OpenAI-compatible endpoint for client-side AI policy.
# These will be the defaults shown in UI
# (UI tab active only if VITE_BROWSER_LLM_ENABLED is true.)
#VITE_LLM_BASE_URL=http://127.0.0.1:1234/v1
#VITE_LLM_API_KEY=
#VITE_LLM_MODEL=mistralai/devstral-small-2-2512

# Timeouts & Delays (ms)

# Max time to wait for AI response before aborting (default 30000ms)
#VITE_LLM_TURN_TIMEOUT_MS=30000

# Minimum time to wait before submitting AI move (simulates "thinking")
#VITE_LLM_MIN_THINK_TIME_MS=300

# Max time to wait for backend to prepare prompt context
#VITE_AI_PROMPT_TIMEOUT_MS=10000

# Debugging

# When true, enables detailed debug logs in the browser console.
#VITE_LLM_LOGGING_ENABLED=false